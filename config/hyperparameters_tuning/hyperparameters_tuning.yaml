defaults:
  - override hydra/hydra_logging: "none"
  - override hydra/job_logging: "custom"

hydra:
  run:
    dir: ./logs/hyperparameters_tuning/${now:%Y-%m-%d}/${now:%H-%M-%S}

data_filepath: "data/processed/" # the / at the end is necessary
base_model_filepath: "models/base"

name: "2nd gradient boost tuning for 095 alpha"
model_type: gb.pkl
metric: "r2"

model_parameters:
  loss: "quantile"
  alpha: 0.95
  n_estimators: 1000
  max_depth: 28
  min_samples_split: 5
  
pipeline_step_names: ["ua_clean", "ma_clean", "col_dropper", "cat_handler", "nan_handler", "final_ct"]
preprocessor_name: "preprocessor"
predictor_name: "predictor"

hyperparameter_tuning:
  name: "gb_opt_095_1"
  param_grid:
    model:
      criterion: ["friedman_mse", "squared_error"]
      # learning_rate: [0.25, 0.1, 0.01, 0.001]
      # n_estimators: [600, 900, 1200, 1500, 1700, 2000]
      # max_depth: [4, 8, 12, 16, 20, 24, 28]
      # n_estimators: [400, 500, 600, 700, 800, 900, 1000]
      # max_depth: [4, 8, 12, 16, 20, 24, 28]
      # min_samples_split: [2, 5, 10, 20]
  cv_no: 5
  random_seed: 2024
  n_jobs: -1
  return_train_score: True
  verbose: 3
  refit: False
